{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pydracor\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk import *\n",
    "import math\n",
    "from scipy.stats import entropy  # provides Shannon entropy and KL divergence\n",
    "import numpy as np    \n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f64ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCText(corpus_name, filter_kwargs=None):\n",
    "    #from pydracor import DraCor, Corpus\n",
    "        \n",
    "    #dracor = DraCor()\n",
    "\n",
    "    # Step 1: Initialize corpus\n",
    "    corpus = Corpus(corpus_name)\n",
    "    metadata = corpus.metadata()\n",
    "    \n",
    "    if filter_kwargs:\n",
    "        corpus = corpus.filter(**filter_kwargs) #corpus became list with play ids\n",
    "    else:\n",
    "        corpus = list(corpus.play_ids())\n",
    "\n",
    "    data={}\n",
    "    names,authors,years,ids,spoken_text=[], [], [], [], []\n",
    "    for i in range(len(corpus)):\n",
    "        #print(i)\n",
    "        play_meta = next(item for item in metadata if item['id'] == corpus[i])\n",
    "        \n",
    "        authors.append(play_meta['first_author'])\n",
    "        years.append(play_meta['year_normalized'])\n",
    "        ids.append(play_meta['id'])\n",
    "        names.append(play_meta['name'])\n",
    "\n",
    "    # Step 4: Use correct corpus in the URL\n",
    "        url = f\"https://dracor.org/api/v1/corpora/{corpus_name}/plays/{names[i]}/spoken-text\"\n",
    "        response = requests.get(url)\n",
    "        spoken_text.append(response.text)\n",
    "        #print(f\"{names} ({corpus_name})\\n\", spoken_text[i][:300])  # Print preview\n",
    "\n",
    "    data[\"names\"] =names\n",
    "    data[\"authors\"]=authors\n",
    "    data[\"ids\"]=ids\n",
    "    data[\"years\"]=years\n",
    "    data[\"spoken_text\"]=spoken_text\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "sh_data = DCText(\"gersh\")\n",
    "wa_data=DCText(\"ger\",{\"authors__name__icontains\": \"Wagner, Richard\"})\n",
    "#print(wa_data[\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe75a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus: wagner's other works\n",
    "urls = [\"https://opera-guide.ch/operas/das+liebesverbot/libretto/de/\",\n",
    "      \"https://opera-guide.ch/operas/die+feen/libretto/de/\",\n",
    "      \"https://opera-guide.ch/operas/rienzi+der+letzte+der+tribunen/libretto/de/\"]\n",
    "\n",
    "\n",
    "\n",
    "def getHTMLSpokenText(url):\n",
    "    #get play name from url\n",
    "    name=re.search(r\"/operas/([^/]+)/libretto/\", url).group(1).replace(\"+\", \"-\")\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Step 1: Extract the div\n",
    "    div = soup.find(\"div\", class_=\"col-lg-7 col-12\")\n",
    "    if not div:\n",
    "        return \"\"\n",
    "\n",
    "    # Step 2: Make a clean copy of the div before we modify it\n",
    "    div_clean = BeautifulSoup(str(div), \"html.parser\").div\n",
    "\n",
    "    # Step 3: Remove <i> tags\n",
    "    for i_tag in div_clean.find_all(\"i\"):\n",
    "        i_tag.decompose()\n",
    "\n",
    "    # Step 4: Get the raw text (before removing uppercase lines)\n",
    "    full_text = div_clean.get_text(separator=\"\\n\")\n",
    "\n",
    "\n",
    "    # Step 5: Trim everything before \"ERSTER AKT\"\n",
    "    match = re.search(\n",
    "    r\"\\b(?:ERSTER?|I\\.)\\s+(?:AKT|AUFZUG|SZENE)\\b\", \n",
    "    full_text, \n",
    "    re.IGNORECASE\n",
    "        )\n",
    "    if match:\n",
    "        full_text = full_text[match.start():]\n",
    "    else:\n",
    "        print(\"Warning: 'ERSTER AKT' not found.\")\n",
    "        return full_text.strip()  # return full uncut version just in case\n",
    "\n",
    "    # Step 6: Remove all-uppercase lines\n",
    "    lines = full_text.splitlines()\n",
    "    clean_lines = [line for line in lines if not line.strip().isupper() and line.strip() != \"\"]\n",
    "    text = \"\\n\".join(clean_lines)\n",
    "\n",
    "    return text.strip(),name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd864d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#merging additional plays to wagner corpus\n",
    "\n",
    "for url in urls:\n",
    "    add_text,add_name = getHTMLSpokenText(url)\n",
    "    wa_data['spoken_text'].append(add_text)\n",
    "    wa_data[\"names\"].append(add_name)\n",
    "    wa_data[\"authors\"].append('Wagner')\n",
    "    wa_data['ids'].append(str('opera_guide_'+str(urls.index(url))))\n",
    "wa_data['years'].extend([1835,1888,1842])\n",
    "#print( wa_data['years'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combineData(*datasets):\n",
    "    \"\"\"\n",
    "    Combines multiple dict datasets with the same keys.\n",
    "    Each key's value from all datasets will be concatenated.\n",
    "    \"\"\"\n",
    "    if not datasets:\n",
    "        return {}\n",
    "    \n",
    "    combined = {}\n",
    "    for key in datasets[0]:\n",
    "        combined[key] = []\n",
    "        for data in datasets:\n",
    "            combined[key] += data[key]\n",
    "    return combined\n",
    "#combined_data = combineData(wa_data,sh_data)\n",
    "#print(combined_data['names'])\n",
    "#combined_data = {key: wa_data[key] + sh_data[key] for key in wa_data}\n",
    "#print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf126a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Romantic baseline:\n",
    "'''\n",
    "Additional Control Group Suggestions — Closer to Wagner’s Genre and Style\n",
    "\n",
    "“Die Räuber” (The Robbers) by Friedrich Schiller (1781)\n",
    "\n",
    "“Euryanthe” (Libretto by Helmina von Chézy, music by Carl Maria von Weber, 1823)\n",
    "\n",
    "“Der Freischütz” (Libretto by Friedrich Kind, music by Carl Maria von Weber, 1821)\n",
    "\n",
    "“Medea” (Play by Franz Grillparzer, 1820)\n",
    "\n",
    "“Penthesilea” (Play by Heinrich von Kleist, 1808)\n",
    "'''\n",
    "control_name=[\"kind-der-freischuetz\",\n",
    "\"schiller-die-raeuber\",\n",
    "\"grillparzer-medea\",\n",
    "\"kleist-penthesilea\"]\n",
    "co_data={'names':[],'authors':[],'ids':[],'years':[],'spoken_text':[]}\n",
    "for name in control_name:\n",
    "    #print(name)\n",
    "    data= DCText(\"ger\",{\"name__contains\":name})\n",
    "    #print(data)\n",
    "    co_data=combineData(data,co_data)\n",
    "\n",
    "print(co_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33527ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize all texts once and store\n",
    "def tokenizeData(data, nlp):\n",
    "    data[\"tokens\"] = [nlp(text) for text in data[\"spoken_text\"]]\n",
    "    return data\n",
    "\n",
    "# Tokenize both datasets\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "wa_data = tokenizeData(wa_data, nlp)\n",
    "sh_data = tokenizeData(sh_data, nlp)\n",
    "co_data=tokenizeData(co_data,nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS Tag N-grams\n",
    "\n",
    "def getPOS(tokens):\n",
    "    return [token.pos_ for token in tokens if token.is_alpha] #if token.is_alpha?\n",
    "\n",
    "def dataPOS(data):\n",
    "    pos_tags=[]\n",
    "    for tokens in data['tokens']:\n",
    "        posed=getPOS(tokens)\n",
    "        pos_tags.append(posed)\n",
    "\n",
    "    data['pos_tags']= pos_tags\n",
    "    return data\n",
    "\n",
    "\n",
    "co_data = dataPOS(co_data)\n",
    "wa_data =dataPOS(wa_data)\n",
    "sh_data =dataPOS(sh_data)\n",
    "#print(co_data['pos_tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data=combineData(wa_data,sh_data,co_data)\n",
    "#print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get min token length for normalisation\n",
    "def findMin(data):\n",
    "    return min(len(doc) for doc in data[\"tokens\"])\n",
    "\n",
    "min_tokens = findMin(combined_data)\n",
    "print(f\"Shortest corpus length (in tokens): {min_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf608751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part1: funtional words\n",
    "# Function Word Frequencies\n",
    "\n",
    "\n",
    "\n",
    "#lemmatizer=nlp.get_pipe(\"lemmatizer\")\n",
    "def getFunWord(text):\n",
    "    return str(\" \".join([token.lemma_ for token in text[:min_tokens] if token.is_stop]).lower()).split()\n",
    "\n",
    "#funWords =getFunWord(wa_data[\"tokens\"][0])\n",
    "\n",
    "\n",
    "#map the frequency\n",
    "#spacy dont have a stop word list\n",
    "def getAllFunLsit(data):\n",
    "    allFunList=[]\n",
    "    for text in data[\"tokens\"]:\n",
    "        text =getFunWord(text)\n",
    "        for word in text:\n",
    "            if word not in allFunList:\n",
    "                allFunList.append(word)\n",
    "    return allFunList\n",
    "\n",
    "all_fun_list = getAllFunLsit(combined_data)\n",
    "\n",
    "\n",
    "#frequency = (wordcount / minlen) * 1000\n",
    "\n",
    "\n",
    "def corpusFreq(data):\n",
    "    corpus_freq={}\n",
    "    for name, tokens in zip(data[\"names\"], data[\"tokens\"]):\n",
    "        fun_Words =getFunWord(tokens)\n",
    "        counts = Counter(fun_Words)\n",
    "        print(name,counts)\n",
    "        freq={\n",
    "            word:(counts[word]/min_tokens)*1000\n",
    "            for word in all_fun_list\n",
    "            if counts[word] > 0\n",
    "        }\n",
    "\n",
    "        corpus_freq[name]=freq\n",
    "    return corpus_freq\n",
    "corpus_freq= corpusFreq(combined_data)\n",
    "print(corpus_freq)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get sorted order by author then year ---\n",
    "sorted_indices = sorted(\n",
    "    range(len(combined_data[\"names\"])),\n",
    "    key=lambda i: (combined_data[\"authors\"][i], combined_data[\"years\"][i])\n",
    ")\n",
    "\n",
    "# --- Reorder combined_data dict first ---\n",
    "for key in combined_data.keys():\n",
    "    combined_data[key] = [combined_data[key][i] for i in sorted_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the dictionary of normalized frequencies into a Pandas DataFrame\n",
    "\n",
    "df_frequencies = pd.DataFrame.from_dict(corpus_freq, orient='index').fillna(0)\n",
    "df_frequencies = df_frequencies.loc[[combined_data[\"names\"][i] for i in range(len(combined_data[\"names\"]))]]\n",
    "df_frequencies['authors'] = combined_data['authors']\n",
    "#print(df_frequencies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KL(P || Q) = sum over all words of P(word) × log(P(word) / Q(word))\n",
    "# using scipy.stats.entrop\n",
    "#overall_KL\n",
    "\n",
    "\n",
    "# Import the KL/entropy utilities from SciPy\n",
    "#from scipy.stats import entropy  # provides Shannon entropy and KL divergence\n",
    "#import numpy as np               # numerical arrays and vectorized operations\n",
    "\n",
    "\n",
    "\n",
    "def norm_count(counts, eps=1e-8):\n",
    "\n",
    "    vec = np.array([counts.get(word, 0.0) + eps for word in all_fun_list], dtype=float)  #Adds a small epsilon to avoid zero probabilities (important for KL divergence).\n",
    "\n",
    "    # Normalize by the total so the vector sums to 1.0 (turn counts into probabilities).\n",
    "    return vec / vec.sum()\n",
    "\n",
    "def symKL(p, q, base=2):\n",
    "    return 0.5 * (entropy(p, qk=q, base=base) + entropy(q, qk=p, base=base))\n",
    "\n",
    "def JS(p, q, base=2):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * entropy(p, m, base=base) + 0.5 * entropy(q, m, base=base)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20659ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_freq_vectors={name:norm_count(counts) for name,counts in corpus_freq.items()}\n",
    "#print(corpus_freq_vectors)\n",
    "\n",
    "# ----------------------------\n",
    "# full pairwise matrix\n",
    "# ----------------------------\n",
    "\n",
    "# Get a stable list of text IDs to index rows/columns of the matrix\n",
    "texts = list(corpus_freq_vectors.keys())      # preserve insertion order (or sort if you prefer)\n",
    "n = len(texts)                    # number of texts being compared\n",
    "\n",
    "# Initialize an n x n matrix of zeros (float64 by default)\n",
    "D = np.zeros((n, n))              # D[i, j] will store the divergence between texts[i] and texts[j]\n",
    "\n",
    "# Fill the upper triangle (i < j) and mirror to keep the matrix symmetric\n",
    "for i in range(n):                # loop over row index i\n",
    "    for j in range(i + 1, n):     # loop over column index j, only for j > i\n",
    "        p = corpus_freq_vectors[texts[i]]     # probability vector for the i-th text\n",
    "        q = corpus_freq_vectors[texts[j]]     # probability vector for the j-th text\n",
    "        d = symKL(p, q)          # compute the divergence (swap in jensen_shannon if preferred)\n",
    "        D[i, j] = d               # set the upper-triangular entry\n",
    "        D[j, i] = d               # mirror to the lower-triangular entry to keep symmetry\n",
    "df_kl = pd.DataFrame(D, index=texts, columns=texts)\n",
    "\n",
    "\n",
    "\n",
    "# Optionally, print or inspect the matrix and its labels\n",
    "#print(\"Texts:\", texts)            # show the order of texts corresponding to rows/columns of D\n",
    "print(\"Divergence matrix (sym. KL, bits):\")  # header for readability\n",
    "         \n",
    "\n",
    "# Create DataFrame from divergence matrix\n",
    "df_kl = pd.DataFrame(D, index=texts, columns=texts)\n",
    "\n",
    "#print(df_kl)\n",
    "plt.figure(figsize=(80, 80))\n",
    "sns.heatmap(df_kl, annot=True, cmap='viridis_r', fmt=\".2f\", linewidths=.5,vmin=0, vmax=2, center=1)\n",
    "plt.title('Divergence matrix')\n",
    "plt.xlabel('Text')\n",
    "plt.ylabel('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea45dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(corpus_freq_vectors.keys())      # preserve insertion order \n",
    "n = len(texts)                    # number of texts being compared\n",
    "\n",
    "# Initialize an n x n matrix of zeros (float64 by default)\n",
    "D = np.zeros((n, n))              # D[i, j] will store the divergence between texts[i] and texts[j]\n",
    "\n",
    "# Fill the upper triangle (i < j) and mirror to keep the matrix symmetric\n",
    "for i in range(n):                # loop over row index i\n",
    "    for j in range(i + 1, n):     # loop over column index j, only for j > i\n",
    "        p = corpus_freq_vectors[texts[i]]     # probability vector for the i-th text\n",
    "        q = corpus_freq_vectors[texts[j]]     # probability vector for the j-th text\n",
    "        d = JS(p, q)          # compute the divergence (swap in jensen_shannon if preferred)\n",
    "        D[i, j] = d               # set the upper-triangular entry\n",
    "        D[j, i] = d               # mirror to the lower-triangular entry to keep symmetry\n",
    "js_df = pd.DataFrame(D, index=texts, columns=texts)\n",
    "print(js_df)\n",
    "plt.figure(figsize=(80, 80))\n",
    "sns.heatmap(js_df, annot=True, cmap='viridis_r', fmt=\".2f\", linewidths=.5,vmin=0, vmax=1, center=0.5)\n",
    "plt.title('JS Divergence matrix')\n",
    "plt.xlabel('Text')\n",
    "plt.ylabel('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da212c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Heatmap Generation ---\n",
    "\n",
    "\n",
    "# --- Compute distances ---\n",
    "# Select only numeric columns (all function word frequency columns)\n",
    "freq_only = df_frequencies.select_dtypes(include=['number'])\n",
    "\n",
    "# Now run pairwise distance on numeric data only\n",
    "#from scipy.spatial.distance import pdist, squareform\n",
    "distances = pdist(freq_only.values, metric='euclidean')\n",
    "\n",
    "distance_matrix = pd.DataFrame(\n",
    "    squareform(distances),\n",
    "    index=df_frequencies.index,\n",
    "    columns=df_frequencies.index\n",
    ")\n",
    "\n",
    "# 1. Compute average profile per author:\n",
    "author_means = df_frequencies.groupby('authors').mean()\n",
    "\n",
    "# 2. Rename the indices so you can easily identify these rows in the heatmap:\n",
    "author_means.index = [f\"{authors}_Avg\" for authors in author_means.index]\n",
    "\n",
    "\n",
    "\n",
    "# 3. Append these average profiles to original DataFrame:\n",
    "df_with_avg = pd.concat([df_frequencies, author_means])\n",
    "\n",
    "distances= pdist(author_means.select_dtypes(include=['number']).values, metric='euclidean')\n",
    "distance_matrix = pd.DataFrame(\n",
    "    squareform(distances), \n",
    "    index=author_means.index, \n",
    "    columns=author_means.index\n",
    ")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(distance_matrix, annot=True, cmap='viridis_r', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Stylometric Distance Heatmap(function word frequency)')\n",
    "plt.xlabel('Text')\n",
    "plt.ylabel('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Recompute the distance matrix (Euclidean) for the extended DataFrame:\n",
    "distances = pdist(df_with_avg.select_dtypes(include=['number']).values, metric='euclidean')\n",
    "\n",
    "distance_matrix = pd.DataFrame(\n",
    "    squareform(distances), \n",
    "    index=df_with_avg.index, \n",
    "    columns=df_with_avg.index\n",
    ")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(120, 100))\n",
    "sns.heatmap(distance_matrix, annot=True, cmap='viridis_r', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Stylometric Distance Heatmap(function word frequency)')\n",
    "plt.xlabel('Text')\n",
    "plt.ylabel('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction: POS Tag Frequencies and N-grams\n",
    "\n",
    "# Function to calculate normalized POS tag frequencies (unigrams)\n",
    "def calculate_pos_frequencies(pos):\n",
    "    pos_count = Counter(pos)\n",
    "    total_tags = sum(pos_count.values())\n",
    "    # Normalize to get proportions/percentages\n",
    "    pos_freq = {tag: count / total_tags for tag, count in pos_count.items()}\n",
    "    return pos_freq\n",
    "\n",
    "# Function to calculate normalized POS tag N-gram frequencies\n",
    "def calculate_pos_ngrams(pos, n):\n",
    "    pos_ngrams = list(ngrams(pos, n))\n",
    "    ngram_count = Counter(pos_ngrams)\n",
    "    total_ngrams = sum(ngram_count.values())\n",
    "    # Normalize to get proportions/probabilities\n",
    "    ngram_freq = {ngram: count / total_ngrams for ngram, count in ngram_count.items()}\n",
    "    return ngram_freq\n",
    "\n",
    "# Store calculated features for each play\n",
    "pos_frequencies = {}\n",
    "pos_bigrams = {}\n",
    "pos_trigrams = {}\n",
    "\n",
    "for i, name in enumerate(combined_data[\"names\"]):\n",
    "    pos_tags = combined_data[\"pos_tags\"][i]\n",
    "    pos_frequencies[name] = calculate_pos_frequencies(pos_tags)\n",
    "    pos_bigrams[name] = calculate_pos_ngrams(pos_tags, n=2)\n",
    "    pos_trigrams[name] = calculate_pos_ngrams(pos_tags, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert  frequency dictionary to a DataFrame\n",
    "tag_df = pd.DataFrame.from_dict(pos_frequencies, orient=\"index\").fillna(0.0)\n",
    "tag_df['authors'] = combined_data['authors']\n",
    "\n",
    "\n",
    "author_means = tag_df.groupby(\"authors\").mean().reset_index()\n",
    "# Drop non-POS columns first if needed\n",
    "pos_columns = [col for col in author_means.columns if col not in ['authors']]\n",
    "\n",
    "# Calculate row-wise percentage for each author\n",
    "author_means_percentage = author_means.copy()\n",
    "author_means_percentage[pos_columns] = author_means_percentage[pos_columns].div(\n",
    "    author_means_percentage[pos_columns].sum(axis=1), axis=0\n",
    ") * 100  # multiply by 100 to get percentage\n",
    "\n",
    "#print(author_means_percentage)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pos_columns = [col for col in author_means_percentage.columns if col != 'authors']\n",
    "df = author_means_percentage.set_index('authors')[pos_columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# stacked bar plot\n",
    "bars = df.plot(kind='bar', stacked=True, ax=ax, colormap='tab20')\n",
    "\n",
    "# annotate each segment with percentage\n",
    "for i, author in enumerate(df.index):\n",
    "    bottom = 0\n",
    "    for pos in pos_columns:\n",
    "        val = df.loc[author, pos]\n",
    "        if val > 0:\n",
    "            ax.text(\n",
    "                i, \n",
    "                bottom + val/2,           # position in the middle of the segment\n",
    "                f\"{val:.1f}%\",            # show one decimal\n",
    "                ha='center', va='center',\n",
    "                fontsize=8,\n",
    "                color='white'             # adjust color for contrast\n",
    "            )\n",
    "            bottom += val\n",
    "\n",
    "plt.ylabel(\"Percentage of POS Tags\")\n",
    "plt.title(\"POS Tag Distribution per Author\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"POS Tag\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d5a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmaps (bigram frequency matrices)\n",
    "# We'll create a heatmap for each author.\n",
    "# First, get a consolidated list of all unique POS tags to ensure consistent matrix dimensions.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "all_unique_tags = sorted(list(set(tag for tags_list in combined_data[\"pos_tags\"] for tag in tags_list)))\n",
    "\n",
    "n_authors = len(combined_data[\"names\"])\n",
    "n_rows, n_cols = 6, 9  # 6x9 grid\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))  # adjust size as needed\n",
    "\n",
    "# Flatten axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, name in zip(axes, combined_data[\"names\"]):\n",
    "    bigram_freqs = pos_bigrams[name]\n",
    "\n",
    "    # Create a matrix for the heatmap\n",
    "    bigram_matrix = pd.DataFrame(0.0, index=all_unique_tags, columns=all_unique_tags)\n",
    "    for (tag1, tag2), freq in bigram_freqs.items():\n",
    "        if tag1 in all_unique_tags and tag2 in all_unique_tags:\n",
    "            bigram_matrix.loc[tag1, tag2] = freq\n",
    "    #print(bigram_matrix)\n",
    "    # Plot heatmap on the specific axis\n",
    "    sns.heatmap(bigram_matrix, annot=False, fmt=\".2f\", cmap=\"Blues\",\n",
    "                linewidths=.5, linecolor='lightgray', cbar=True, ax=ax, vmin=0, vmax=0.01, center=0.003)\n",
    "    ax.set_title(f'{name}', fontsize=10)\n",
    "    ax.set_xlabel('Second Tag', fontsize=8)\n",
    "    ax.set_ylabel('First Tag', fontsize=8)\n",
    "\n",
    "# Turn off any unused subplots\n",
    "for i in range(len(combined_data[\"names\"]), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56497707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi gram z score\n",
    "\n",
    "# Gather all unique bigrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#print(pos_bigrams)\n",
    "# Gather all unique bigrams and convert to a list\n",
    "all_bigrams=[]\n",
    "for text_name, bigram_counts in pos_bigrams.items():\n",
    "    for bigram, freq in bigram_counts.items():\n",
    "        # bigram is a tuple like ('AUX', 'PRON')\n",
    "        bi_str = f\"{bigram[0]}-{bigram[1]}\"  # convert tuple to string\n",
    "        if bi_str not in all_bigrams:\n",
    "            all_bigrams.append(bi_str)\n",
    "        #print(bi_str, freq)\n",
    "\n",
    "#print(all_bigrams)\n",
    "# Create a DataFrame: rows=texts, columns=bigrams\n",
    "df = pd.DataFrame(index=pos_bigrams.keys(), columns=all_bigrams).fillna(0.0)\n",
    "\n",
    "\n",
    "for text, counts in pos_bigrams.items():\n",
    "    for bigram, freq in counts.items():\n",
    "        bi_str = f\"{bigram[0]}-{bigram[1]}\"  # convert tuple to string\n",
    "        df.loc[text, bi_str] = freq\n",
    "\n",
    "#print(df)\n",
    "# Compute z-scores for each bigram\n",
    "z_scores = df.copy()\n",
    "z_scores['authors']=combined_data['authors']\n",
    "for col in df.columns:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std(ddof=0)  # population std; use ddof=1 for sample std\n",
    "    if std > 0:\n",
    "        z_scores[col] = (df[col] - mean) / std\n",
    "    else:\n",
    "        z_scores[col] = 0.0  # if no variance, z-score is zero\n",
    "\n",
    "print(z_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = z_scores.reset_index(drop=True)  # drop old index\n",
    "z_scores['text'] = combined_data['names']   # create a proper text/play column\n",
    "print(z_scores.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b67e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# melt\n",
    "df_long = z_scores.melt(\n",
    "    id_vars=['text', 'authors'], \n",
    "    var_name='bigram', \n",
    "    value_name='z_score'\n",
    ")\n",
    "\n",
    "# Split bigram strings into two tags\n",
    "df_long[['tag1','tag2']] = df_long['bigram'].str.split('-', expand=True)\n",
    "\n",
    "print(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24408ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "threshold = 2\n",
    "df_sig = df_long[df_long['z_score'].abs() >= threshold]\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "sns.scatterplot(\n",
    "    data=df_sig,\n",
    "    x='tag1',\n",
    "    y='tag2',\n",
    "    hue='text',      # color by play/text\n",
    "    style='authors', # shape by author\n",
    "    s=500,\n",
    "    size='z_score',\n",
    "    sizes=(200, 500),\n",
    "    palette='tab20',\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Move legend below and horizontal\n",
    "plt.legend(\n",
    "    title='Play',\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.15),  # position below plot\n",
    "    ncol=5,                        # number of columns in legend\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "plt.xlabel(\"First POS Tag\")\n",
    "plt.ylabel(\"Second POS Tag\")\n",
    "plt.title(\"POS Bigram Z-Scores by Text and Author\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create facet grid\n",
    "\n",
    "# df columns: bigram, authors, text, z_score\n",
    "threshold = 2\n",
    "df_sig = df_long[df_long['z_score'].abs() >= threshold]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    data=df_sig,  \n",
    "    col=\"bigram\",\n",
    "    col_wrap=16,        # number of facets per row\n",
    "    height=4,\n",
    "    sharey=False,\n",
    "    aspect=1\n",
    ")\n",
    "\n",
    "# Map stripplot to each facet\n",
    "g.map_dataframe(\n",
    "    sns.stripplot,\n",
    "    x=\"authors\",\n",
    "    y=\"z_score\",\n",
    "    hue=\"text\",        # color by play name\n",
    "    dodge=True,\n",
    "    alpha=0.7,\n",
    "    size=20\n",
    ")\n",
    "\n",
    "# Move legend below\n",
    "g.add_legend(title=\"Play\", ncol=len(z_scores[\"text\"].unique()))\n",
    "g.legend.set_bbox_to_anchor((0.5, -0.05))\n",
    "g.legend.set_loc(\"lower center\")\n",
    "\n",
    "# Rotate author labels\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout(w_pad=1)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e5f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "\n",
    "\n",
    "# Function to create and plot a NetworkX graph for POS bigrams\n",
    "import networkx as nx\n",
    "def plot_pos_network(name, bigram_frequencies, all_tags):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes (all unique POS tags)\n",
    "    G.add_nodes_from(all_tags)\n",
    "\n",
    "    # Add edges with weights (bigram frequencies)\n",
    "    for (tag1, tag2), weight in bigram_frequencies.items():\n",
    "        if tag1 in all_tags and tag2 in all_tags:\n",
    "            G.add_edge(tag1, tag2, weight=weight)\n",
    "\n",
    "    # Filter out edges with very low frequency to reduce clutter for visualization\n",
    "    threshold = 0.0005 # Example threshold for normalized frequencies\n",
    "    filtered_edges = [\n",
    "    (u, v, d) for u, v, d in G.edges(data=True) if d[\"weight\"] >= threshold\n",
    "        ]\n",
    "    \n",
    "    # Create a new graph with only filtered edges for cleaner visualization\n",
    "    G_filtered = nx.DiGraph()\n",
    "    G_filtered.add_nodes_from(G.nodes()) # Add all nodes back, even if they have no strong connections\n",
    "    G_filtered.add_edges_from(filtered_edges)\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    # Use a spring layout for better node distribution\n",
    "    pos = nx.spring_layout(G_filtered, k=0.8, iterations=50) # k adjusts optimal distance between nodes\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G_filtered, pos, node_size=2500, node_color='skyblue', alpha=0.9)\n",
    "\n",
    "    # Draw edges, with width proportional to weight\n",
    "    edge_weights = [d['weight'] for u, v, d in G_filtered.edges(data=True)]\n",
    "    if edge_weights: # Avoid division by zero if no edges\n",
    "        max_weight = max(edge_weights)\n",
    "        edge_widths = [w / max_weight * 7 for w in edge_weights] # Scale width for visibility\n",
    "    else:\n",
    "        edge_widths = []\n",
    "\n",
    "    nx.draw_networkx_edges(G_filtered, pos, width=edge_widths, alpha=0.6, edge_color='gray', arrows=True, arrowsize=20)\n",
    "\n",
    "    # Draw node labels\n",
    "    nx.draw_networkx_labels(G_filtered, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "    plt.title(f'POS Tag Transition Network for {name}', size=18)\n",
    "    plt.axis('off') # Hide axes\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot networks for each author\n",
    "for name in combined_data[\"names\"]:\n",
    "    plot_pos_network(name, pos_bigrams[author], all_unique_tags)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------- 1. Combine POS uni-, bi-, and trigram frequencies ----------\n",
    "X = {}\n",
    "for name in combined_data['names']:\n",
    "    X[name] = copy.deepcopy(pos_frequencies[name])      # Start with unigrams\n",
    "    X[name].update(pos_bigrams[name])                   # Add bigrams\n",
    "    X[name].update(pos_trigrams[name])                  # Add trigrams\n",
    "\n",
    "# ---------- 2. Normalize keys: turn tuples into strings ----------\n",
    "def string_key(k):\n",
    "    if isinstance(k, str):\n",
    "        return k\n",
    "    return \">\".join(k)  # Join POS tags with '>' to mark order\n",
    "\n",
    "X_str_keys = {}\n",
    "for name, feats in X.items():\n",
    "    X_str_keys[name] = {string_key(k): v for k, v in feats.items()}\n",
    "\n",
    "# ---------- 3. Build DataFrame ----------\n",
    "df_features = pd.DataFrame.from_dict(X_str_keys, orient=\"index\").fillna(0)\n",
    "\n",
    "print(\"Feature matrix shape:\", df_features.shape)\n",
    "print(\"Sample columns:\", df_features.columns[:-10])\n",
    "\n",
    "# ---------- 4. Apply PCA ----------\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df_features)\n",
    "\n",
    "pca_df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df['authors']=combined_data['authors']\n",
    "pca_df['years']=combined_data['years']\n",
    "pca_df['names']=df_features.index\n",
    "\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31549da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- 5. Scatter plot ----------\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.scatterplot(\n",
    "    data=pca_df,\n",
    "    x=\"PC1\", y=\"PC2\",\n",
    "    hue=\"years\",                # Color points by year\n",
    "    palette=\"viridis\",\n",
    "    style='authors',\n",
    "    \n",
    "    s=300\n",
    ")\n",
    "\n",
    "# Add name labels for each point\n",
    "for i, row in pca_df.iterrows():\n",
    "    plt.text(\n",
    "        row[\"PC1\"] + 0.002,      # small horizontal offset\n",
    "        row[\"PC2\"] + 0.002,      # small vertical offset\n",
    "        row[\"names\"],            # column in pca_df with the play name\n",
    "        fontsize=10,\n",
    "        color=\"black\"\n",
    "    )\n",
    "\n",
    "\n",
    "plt.title(\"PCA of POS N-gram Frequencies\", fontsize=16)\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "plt.axhline(0, color=\"grey\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.axvline(0, color=\"grey\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.legend(title=\"plays\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
