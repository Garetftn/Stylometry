{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import kl_div\n",
    "from scipy.stats import entropy  # provides Shannon entropy and KL divergence\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import networkx as nx\n",
    "from nltk import *\n",
    "import math\n",
    "import numpy as np    \n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00542c0e",
   "metadata": {},
   "source": [
    "libraries:\n",
    "spacy: tokenize corpus, stopwords,lemmazation, text cleaning, pos tagging\n",
    "requests: access dracor and opera-guide.ch\n",
    "beautifulsoup: html sparsing for getting main text and metadata from opera guide\n",
    "pandas: data storing and manupilation\n",
    "collections, counter: count words and frequencies\n",
    "plt: visualisation\n",
    "seaborn: visualisation\n",
    "scipy: entropy/kl_div: kl divergence; pdist,quareform:metric distance\n",
    "sklearn:pca,cosine_similarity\n",
    "nltk: generate n grams\n",
    "math\n",
    "numpy\n",
    "copy: dictionary munplation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89346b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize corpus from dracor outside the function to aviod repeat loading\n",
    "\n",
    "from pydracor import  Corpus\n",
    "sh_corpus = Corpus(\"gersh\")\n",
    "ger_corpus = Corpus(\"ger\")\n",
    "ger_meta = ger_corpus.metadata()\n",
    "sh_meta = sh_corpus.metadata()\n",
    "print(ger_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f64ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCText(corpus_name, filter_kwargs=None):\n",
    "    from pydracor import DraCor, Corpus\n",
    "\n",
    "    if corpus_name == \"gersh\":\n",
    "        corpus = sh_corpus\n",
    "        metadata = sh_meta\n",
    "    else:\n",
    "        corpus = ger_corpus \n",
    "        metadata = ger_meta\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    if filter_kwargs:\n",
    "        corpus = corpus.filter(**filter_kwargs) #corpus became list with play ids\n",
    "        #print(corpus)\n",
    "    else:\n",
    "        corpus = list(corpus.play_ids())\n",
    "\n",
    "    data={}\n",
    "    names,authors,years,ids,spoken_text=[], [], [], [], []\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        \n",
    "        #print(i)\n",
    "        play_meta = next(item for item in metadata if item['id'] == corpus[i])\n",
    "\n",
    "        if int(play_meta['word_count_text']) < 7400:\n",
    "            continue\n",
    "\n",
    "        author_count = len(set(authors))\n",
    "        if author_count == 0:\n",
    "            author_count = 1\n",
    "        if  author_count > 5 and len(names) == 25:  # stop if we already have 5 authors or 5*5 plays\n",
    "            break \n",
    "\n",
    "        author = play_meta['first_author']\n",
    "\n",
    "\n",
    "        if authors.count(author) >= 10: #each author has maxium 5 plays\n",
    "            continue\n",
    "        \n",
    "\n",
    "        \n",
    "        authors.append(play_meta['first_author'])\n",
    "        years.append(play_meta['year_normalized'])\n",
    "        ids.append(play_meta['id'])\n",
    "        names.append(play_meta['name'])\n",
    "\n",
    "    # use correct corpus in the URL\n",
    "        url = f\"https://dracor.org/api/v1/corpora/{corpus_name}/plays/{play_meta['name']}/spoken-text\"\n",
    "        response = requests.get(url)\n",
    "        spoken_text.append(response.text)\n",
    "        #print(f\"{names} ({corpus_name})\\n\", spoken_text[i][:30])  # Print preview\n",
    "\n",
    "    data[\"names\"] =names\n",
    "    data[\"authors\"]=authors\n",
    "    data[\"ids\"]=ids\n",
    "    data[\"years\"]=years\n",
    "    data[\"spoken_text\"]=spoken_text\n",
    "\n",
    "    print(len(set(data['authors'])))\n",
    "    print(len(set(data['names'])))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7fff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = frozenset([\n",
    "    \"Romeo und Julia\",\n",
    "    \"Der Sturm\",\n",
    "    \"Hamlet. Prinz von Dänemark\",\n",
    "    \"Maß für Maß\",\n",
    "    \"König Lear\",\n",
    "    \"Macbeth\",\n",
    "    \"Ein Sommernachtstraum\",\n",
    "    \"Othello\",\n",
    "    \"Wie es euch gefällt\",\n",
    "    \"König Johann\"\n",
    "])\n",
    "\n",
    "sh_data = DCText(\"gersh\",{'title__in':titles})\n",
    "\n",
    "wa_data=DCText(\"ger\",{\"authors__name__icontains\": \"Wagner, Richard\"})\n",
    "\n",
    "print(sh_data[\"names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc42fe",
   "metadata": {},
   "source": [
    "fuction1: get text from dracor through pydracor and dracor api\n",
    "saves all the plays that fit the conditionaly filter in one dictionary: data ={names:[play names to be used later in dracor api],authors:[author names],years:[normolised years],ids:[play ids],spoken_text=[]}\n",
    "time effeciency: access meta data of the whole german corpus and get spoken text through pydracor takes too much times. solutions to that are first: move the lines of accessing the metadata metadata = corpus.metadata()\n",
    "out from the funtion so it will only be pulled once; use dracor api to get the spoken text which is way faster beacuse it pulls the text directly through website. doing so saves plenty of runtime in testing and final proformence. to access the desired plays through api requires the play names. playnames are saved in metadata from pycor. thus we can realise the final funtion through a intergration: query the metadata through pydracor and get the list of target playnames, then get the sopken test using dracor api.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe75a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wagner's other works\n",
    "urls = [\"https://opera-guide.ch/operas/das+liebesverbot/libretto/de/\",\n",
    "      \"https://opera-guide.ch/operas/die+feen/libretto/de/\",\n",
    "      \"https://opera-guide.ch/operas/rienzi+der+letzte+der+tribunen/libretto/de/\"]\n",
    "\n",
    "\n",
    "\n",
    "def getHTMLSpokenText(url):\n",
    "    # get play name from url\n",
    "    name=re.search(r\"/operas/([^/]+)/libretto/\", url).group(1).replace(\"+\", \"-\")\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Step 1: Extract the div\n",
    "    div = soup.find(\"div\", class_=\"col-lg-7 col-12\")\n",
    "    if not div:\n",
    "        return \"\"\n",
    "\n",
    "    # Step 2: Make a clean copy of the div before modifing\n",
    "    div_clean = BeautifulSoup(str(div), \"html.parser\").div\n",
    "\n",
    "    # Step 3: Remove <i> tags\n",
    "    for i_tag in div_clean.find_all(\"i\"):\n",
    "        i_tag.decompose()\n",
    "\n",
    "    # Step 4: Get the raw text (before removing uppercase lines)\n",
    "    full_text = div_clean.get_text(separator=\"\\n\")\n",
    "\n",
    "\n",
    "    # Step 5: Trim everything before \"ERSTER AKT\"\n",
    "    match = re.search(\n",
    "    r\"\\b(?:ERSTER?|I\\.)\\s+(?:AKT|AUFZUG|SZENE)\\b\", \n",
    "    full_text, \n",
    "    re.IGNORECASE\n",
    "        )\n",
    "    if match:\n",
    "        full_text = full_text[match.start():]\n",
    "    else:\n",
    "        print(\"Warning: 'ERSTER AKT' not found.\")\n",
    "        return full_text.strip()  # return full uncut version just in case\n",
    "\n",
    "    # Step 6: Remove all-uppercase lines\n",
    "    lines = full_text.splitlines()\n",
    "    clean_lines = [line for line in lines if not line.strip().isupper() and line.strip() != \"\"]\n",
    "    text = \"\\n\".join(clean_lines)\n",
    "\n",
    "    return text.strip(),name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14b77a",
   "metadata": {},
   "source": [
    "2.get adiitional plays from opera guide website:\n",
    "3 plays from wagner which are not predent in Dracor are listed in another website. By html prasing through beautiful soup we can access the texts. \n",
    "\n",
    "after close inspection, we can notice that all texts are stored in <div, class_=\"col-lg-7 col-12\"> tags. \n",
    "apart from the spoken text we need, there are:\n",
    "1.there areadditional information before actual text starts(names, summaries), text starts with'FIRST AKT', or'I. Aufzug/SEZNE' in different plays' cases.\n",
    "2.all the character names are in cap and spereate lines with the spoken text. \n",
    "3.stage directions below charater names in <i> tags.\n",
    "\n",
    "solution:\n",
    "2. use .decompose() from bf4 to remove all stage direction\n",
    "1. find\"first akt\" as a marker through regex and get the main text throuh .start() from bf4 (.decompose() tags before \"first akt\" element will cause critical problem that destories the whole tree and the function will return \"\" as result)\n",
    "3.use regex to find all cap charcters(only names in this case) then remove them.\n",
    "\n",
    "addtionally to align with previous DCText() function we gather the play names from the url directly at the same time. the result of the meta data can be included manually most effciently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd864d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#merging additional plays to wagner corpus\n",
    "\n",
    "for url in urls:\n",
    "    add_text,add_name = getHTMLSpokenText(url)\n",
    "    wa_data['spoken_text'].append(add_text)\n",
    "    wa_data[\"names\"].append(add_name)\n",
    "    wa_data[\"authors\"].append('Wagner')\n",
    "    wa_data['ids'].append(str('opera_guide_'+str(urls.index(url))))\n",
    "wa_data['years'].extend([1835,1888,1842])\n",
    "#print( wa_data['years'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combineData(*datasets):\n",
    "\n",
    "    #combines multiple dict datasets with the same keys\n",
    "\n",
    "    if not datasets:\n",
    "        return {}\n",
    "    \n",
    "    combined = {}\n",
    "    for key in datasets[0]:\n",
    "        combined[key] = []\n",
    "        for data in datasets:\n",
    "            combined[key] += data[key]\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get control sets fromt the same era\n",
    "control = frozenset([\n",
    "    \"Schiller\",\n",
    "    \"Grillparzer\",\n",
    "    \"Hebbel\",\n",
    "])\n",
    "co_data={'names':[],'authors':[],'ids':[],'years':[],'spoken_text':[]}\n",
    "for name in control:\n",
    "    #print(name)\n",
    "    data= DCText(\"ger\",{\"authors__name__icontains\":name})\n",
    "    #print(data)\n",
    "    co_data=combineData(data,co_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jon={'names':[],'authors':[],'ids':[],'years':[],'spoken_text':[]}\n",
    "jon= DCText(\"ger\",{\"name__contains\":\"schlegel-jon\"})\n",
    "co_data=combineData(jon,co_data)\n",
    "#print(co_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33527ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenize all texts in courpus dictionary once and store\n",
    "def tokenizeData(data, nlp):\n",
    "    data[\"tokens\"] = [nlp(text) for text in data[\"spoken_text\"]]\n",
    "    return data\n",
    "\n",
    "# tokenize all datasets\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "wa_data = tokenizeData(wa_data, nlp)\n",
    "sh_data = tokenizeData(sh_data, nlp)\n",
    "co_data =  tokenizeData(co_data,nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and store POS Tag in dictionary\n",
    "\n",
    "def getPOS(tokens):\n",
    "    return [token.pos_ for token in tokens if token.is_alpha] #if token.is_alpha?\n",
    "\n",
    "def dataPOS(data):\n",
    "    pos_tags=[]\n",
    "    for tokens in data['tokens']:\n",
    "        posed=getPOS(tokens)\n",
    "        pos_tags.append(posed)\n",
    "\n",
    "    data['pos_tags']= pos_tags\n",
    "    return data\n",
    "\n",
    "\n",
    "co_data = dataPOS(co_data)\n",
    "wa_data =dataPOS(wa_data)\n",
    "sh_data =dataPOS(sh_data)\n",
    "#print(co_data['pos_tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data=combineData(wa_data,sh_data,co_data)\n",
    "#print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get min token length for normalisation\n",
    "def findMin(data):\n",
    "    return min(len(doc) for doc in data[\"tokens\"])\n",
    "\n",
    "min_tokens = findMin(combined_data)\n",
    "print(f\"Shortest corpus length (in tokens): {min_tokens}\")\n",
    "min_tokens = findMin(wa_data)\n",
    "print(f\"Shortest corpus length (in tokens): {min_tokens}\")\n",
    "min_tokens = findMin(sh_data)\n",
    "print(f\"Shortest corpus length (in tokens): {min_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cfe998",
   "metadata": {},
   "source": [
    "normoalise corpus lenth:Findmin() find out minium lenth in the entrie corpus. all the plays will be trimed into same lenth to aviod bias when nesseary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838e571",
   "metadata": {},
   "source": [
    "function words kl divergence:\n",
    "1.get all word count from every play/ every author\n",
    "2.normalized them so that the components summed to 1\n",
    "2.took each of these normalized vectors to be the feature vector for the corresponding author/play\n",
    "4.apply kl then similarity to dataframe.\n",
    "5.visuisation in heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf608751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Word Frequencies\n",
    "def getFunWord(text):\n",
    "    return str(\" \".join([token.lemma_ for token in text[:min_tokens] if token.is_stop]).lower()).split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016975d4",
   "metadata": {},
   "source": [
    "map out all the function words in a play, stored them as list , each word is one element, and only the function words with in the minlen will be gathered.\n",
    "spacy doesnt have a fix stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78002dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def corpusFreq(data):\n",
    "    corpus_count={}\n",
    "    for name, tokens in zip(data[\"names\"], data[\"tokens\"]):\n",
    "        fun_Words =getFunWord(tokens)\n",
    "        counts = Counter(fun_Words)\n",
    "        print(name,counts)\n",
    "\n",
    "        corpus_count[name]=counts\n",
    "    return corpus_count\n",
    "corpus_count= corpusFreq(combined_data)\n",
    "print(corpus_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca241b",
   "metadata": {},
   "source": [
    "corpus_freq()returns a nested dictionary, key is play name and value will be a collection of funtion words and thier counts in value pairs. the countering and storing the values are easily done by counter() from collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_count = pd.DataFrame.from_dict(corpus_count, orient='index').fillna(0) #Builds  count matrix, row = play, column = a word/feature.\n",
    "#KL involves log(P/Q). If P>0 and Q=0, KL explodes to ∞. Smoothing prevents division by zero and infinite values. but if .fillna(ep), all the counts will be 1 eps short.\n",
    "df_count['authors']=combined_data['authors']\n",
    "df_avg=df_count.groupby(['authors']).sum()\n",
    "eps=1e-8\n",
    "df_avg += eps\n",
    "df_prob = df_avg.div(df_avg.sum(axis=1), axis=0)\n",
    "\n",
    "#L1-normalize to make probabilities divide by the sum so the vector sums to 1\n",
    "\n",
    "\n",
    "print(df_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09d56e",
   "metadata": {},
   "source": [
    "next setp will be the first data stastical method on funtionword frequency. kl direvence .\n",
    "first prepare the data in data frame. Builds  count matrix, row = play, column = a word/feature. we wan to see the performance on author level, so we group all the texts by author name and sum up all the word counts from single plays. \n",
    "#KL involves log(P/Q). If P>0 and Q=0, KL explodes to ∞. Smoothing prevents division by zero and infinite values. but if .fillna(ep), all the counts will be 1 eps short.\n",
    "second, different author has diffrenet total number of plays and to calculate kl , datas must sum up to 1. \n",
    "so Division makes each row sum to 1 → normalized probability distribution of words per author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef908a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kl(df):\n",
    "    index = df.index\n",
    "    kl_matrix = pd.DataFrame(index=index, columns=index, dtype=float)\n",
    "    for a1 in index:\n",
    "        for a2 in index:\n",
    "            p = df.loc[a1]\n",
    "            q = df.loc[a2]\n",
    "            kl_matrix.loc[a1, a2] = kl_div(p, q).sum() # KL(a1 || a2)\n",
    "    return kl_matrix\n",
    "\n",
    "kl_matrix = kl(df_prob)\n",
    "print(kl_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848304ee",
   "metadata": {},
   "source": [
    "KL with spicy:\n",
    "https://stackoverflow.com/questions/63369974/3-functions-for-computing-relative-entropy-in-scipy-whats-the-difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(df):\n",
    "    sigma = 0.5\n",
    "    similarity_matrix = np.exp(-df / sigma)\n",
    "\n",
    "   \n",
    "    similarity_matrix = pd.DataFrame(similarity_matrix, index=df.index, columns=df.columns)\n",
    "    return similarity_matrix\n",
    "\n",
    "similarity_matrix = sim(kl_matrix)\n",
    "print(similarity_matrix)\n",
    "\n",
    "\n",
    "# heatmap visulisation:\n",
    "#print(df_kl)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(similarity_matrix, annot=True, cmap='viridis_r', fmt=\".2f\", linewidths=.5,vmin=0.5, vmax=1)\n",
    "plt.title('Similarity Matrix based on KL Divergence')\n",
    "plt.xlabel('Author')\n",
    "plt.ylabel('Author')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcbf21",
   "metadata": {},
   "source": [
    "convert kl values to similarities according to Hughes et al.\n",
    "the value will be in [0,1], the higher the more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66220ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kl divergence of each plays\n",
    "df =df_count.select_dtypes(include=['number'])\n",
    "df += eps\n",
    "df_all_prob = df.div(df.sum(axis=1), axis=0)\n",
    "\n",
    "#print(df_all_prob)\n",
    "\n",
    "\n",
    "kl_matrix_all = kl(df_all_prob)\n",
    "#print(kl_matrix)\n",
    "\n",
    "similarity_matrix_all= sim(kl_matrix_all)\n",
    "\n",
    "plt.figure(figsize=(80, 80))\n",
    "sns.heatmap(similarity_matrix_all, annot=True, cmap='viridis_r', fmt=\".2f\", linewidths=.5,vmin=0, vmax=1)\n",
    "plt.title('Similarity Matrix based on KL Divergence')\n",
    "plt.xlabel('plays')\n",
    "plt.ylabel('palys')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f1fea",
   "metadata": {},
   "source": [
    "\n",
    "pos uni gram:</br>\n",
    "1.extract all pos for each play</br>\n",
    "2.group df by author and calulate the means for each pos tag frequency under same author</br>\n",
    "3.calculate proportion of each tags for each author in dataframe</br>\n",
    "4.stacked bar plot per auhtor</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction: POS Tag Frequencies and N-grams\n",
    "\n",
    "#function to calculate normalized POS tag frequencies (unigrams)\n",
    "def calculate_pos_frequencies(pos):\n",
    "    pos_count = Counter(pos)\n",
    "    total_tags = sum(pos_count.values())\n",
    "    # Normalize to get proportions/percentages\n",
    "    pos_freq = {tag: count / total_tags for tag, count in pos_count.items()}\n",
    "    return pos_freq\n",
    "\n",
    "# function to calculate normalized POS tag N-gram frequencies\n",
    "def calculate_pos_ngrams(pos, n):\n",
    "    pos_ngrams = list(ngrams(pos, n))\n",
    "    ngram_count = Counter(pos_ngrams)\n",
    "    total_ngrams = sum(ngram_count.values())\n",
    "    # Normalize to get proportions/probabilities\n",
    "    ngram_freq = {ngram: count / total_ngrams for ngram, count in ngram_count.items()}\n",
    "    return ngram_freq\n",
    "\n",
    "# store calculated features for each play\n",
    "pos_frequencies = {}\n",
    "pos_bigrams = {}\n",
    "pos_trigrams = {}\n",
    "\n",
    "for i, name in enumerate(combined_data[\"names\"]):\n",
    "    pos_tags = combined_data[\"pos_tags\"][i]\n",
    "    pos_frequencies[name] = calculate_pos_frequencies(pos_tags)\n",
    "    pos_bigrams[name] = calculate_pos_ngrams(pos_tags, n=2)\n",
    "    pos_trigrams[name] = calculate_pos_ngrams(pos_tags, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert  frequency dictionary to a DataFrame\n",
    "tag_df = pd.DataFrame.from_dict(pos_frequencies, orient=\"index\").fillna(0.0)\n",
    "tag_df['authors'] = combined_data['authors']\n",
    "\n",
    "\n",
    "author_means = tag_df.groupby(\"authors\").mean().reset_index()\n",
    "# drop non-POS columns first\n",
    "pos_columns = [col for col in author_means.columns if col not in ['authors']]\n",
    "\n",
    "# calculate row-wise percentage for each author\n",
    "author_means_percentage = author_means.copy()\n",
    "author_means_percentage[pos_columns] = author_means_percentage[pos_columns].div(\n",
    "    author_means_percentage[pos_columns].sum(axis=1), axis=0\n",
    ") * 100  # multiply by 100 to get percentage\n",
    "\n",
    "#print(author_means_percentage)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pos_columns = [col for col in author_means_percentage.columns if col != 'authors']\n",
    "df = author_means_percentage.set_index('authors')[pos_columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# stacked bar plot\n",
    "bars = df.plot(kind='bar', stacked=True, ax=ax, colormap='tab20')\n",
    "\n",
    "# annotate each segment with percentage\n",
    "for i, author in enumerate(df.index):\n",
    "    bottom = 0\n",
    "    for pos in pos_columns:\n",
    "        val = df.loc[author, pos]\n",
    "        if val > 0:\n",
    "            ax.text(\n",
    "                i, \n",
    "                bottom + val/2,           # position in the middle of the segment\n",
    "                f\"{val:.1f}%\",            # show one decimal\n",
    "                ha='center', va='center',\n",
    "                fontsize=8,\n",
    "                color='white'         \n",
    "            )\n",
    "            bottom += val\n",
    "\n",
    "plt.ylabel(\"Percentage of POS Tags\")\n",
    "plt.title(\"POS Tag Distribution per Author\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"POS Tag\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df65b22",
   "metadata": {},
   "source": [
    "POS bigram:</br>\n",
    "1.plot a heat map for each play based on bigram frequency, the differencies are hard to spot</br>\n",
    "2.calculate z-score to find out the distinguisble bi gram usage among all plays</br>\n",
    "3.Compute cosine similarity to generate dendrogram</br>\n",
    "4.Plot dendrogram</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d5a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmaps (bigram frequency matrices)\n",
    "# create a heatmap for each author\n",
    "\n",
    "all_unique_tags = sorted(list(set(tag for tags_list in combined_data[\"pos_tags\"] for tag in tags_list)))\n",
    "\n",
    "n_authors = len(combined_data[\"names\"])\n",
    "n_rows, n_cols = 6, 9  # 6x9 grid\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))  # adjust size as needed\n",
    "\n",
    "# flatten axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, name in zip(axes, combined_data[\"names\"]):\n",
    "    bigram_freqs = pos_bigrams[name]\n",
    "\n",
    "    # create a matrix for the heatmap\n",
    "    bigram_matrix = pd.DataFrame(0.0, index=all_unique_tags, columns=all_unique_tags)\n",
    "    for (tag1, tag2), freq in bigram_freqs.items():\n",
    "        if tag1 in all_unique_tags and tag2 in all_unique_tags:\n",
    "            bigram_matrix.loc[tag1, tag2] = freq\n",
    "    #print(bigram_matrix)\n",
    "    \n",
    "    # plot heatmap on the specific axis\n",
    "    sns.heatmap(bigram_matrix, annot=False, fmt=\".2f\", cmap=\"Blues\",\n",
    "                linewidths=.5, linecolor='lightgray', cbar=True, ax=ax, vmin=0, vmax=0.01, center=0.003)\n",
    "    ax.set_title(f'{name}', fontsize=10)\n",
    "    ax.set_xlabel('Second Tag', fontsize=8)\n",
    "    ax.set_ylabel('First Tag', fontsize=8)\n",
    "\n",
    "# turn off any unused subplots\n",
    "for i in range(len(combined_data[\"names\"]), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56497707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi gram z score\n",
    "\n",
    "# gather all unique bigrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#print(pos_bigrams)\n",
    "# Gather all unique bigrams and convert to a list\n",
    "all_bigrams=[]\n",
    "for text_name, bigram_counts in pos_bigrams.items():\n",
    "    for bigram, freq in bigram_counts.items():\n",
    "        # bigram is a tuple like ('AUX', 'PRON')\n",
    "        bi_str = f\"{bigram[0]}-{bigram[1]}\"  # convert tuple to string\n",
    "        if bi_str not in all_bigrams:\n",
    "            all_bigrams.append(bi_str)\n",
    "        #print(bi_str, freq)\n",
    "\n",
    "#print(all_bigrams)\n",
    "# create a DataFrame: rows=texts, columns=bigrams\n",
    "df = pd.DataFrame(index=pos_bigrams.keys(), columns=all_bigrams).fillna(0.0)\n",
    "\n",
    "\n",
    "for text, counts in pos_bigrams.items():\n",
    "    for bigram, freq in counts.items():\n",
    "        bi_str = f\"{bigram[0]}-{bigram[1]}\"  # convert tuple to string\n",
    "        df.loc[text, bi_str] = freq\n",
    "\n",
    "#print(df)\n",
    "# compute z-scores for each bigram\n",
    "z_scores = df.copy()\n",
    "z_scores['authors']=combined_data['authors']\n",
    "for col in df.columns:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std(ddof=0)  # population std; use ddof=1 for sample std\n",
    "    if std > 0:\n",
    "        z_scores[col] = (df[col] - mean) / std\n",
    "    else:\n",
    "        z_scores[col] = 0.0  # if no variance, z-score is zero\n",
    "\n",
    "print(z_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = z_scores.reset_index(drop=True)  # drop old index\n",
    "z_scores['text'] = combined_data['names']   # create a proper text/play column\n",
    "print(z_scores.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea47312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select bigram columns\n",
    "bigram_cols = [col for col in z_scores.columns if '-' in col]\n",
    "X = z_scores[bigram_cols].fillna(0)\n",
    "\n",
    "# compute cosine similarity\n",
    "sim_matrix = cosine_similarity(X)\n",
    "\n",
    "# convert to distance\n",
    "dist_matrix = 1 - sim_matrix\n",
    "\n",
    "# ensure diagonal is exactly zero\n",
    "np.fill_diagonal(dist_matrix, 0)\n",
    "\n",
    "# flatten for linkage\n",
    "dist_condensed = squareform(dist_matrix)\n",
    "\n",
    "\n",
    "# hierarchical clustering\n",
    "Z = linkage(dist_condensed, method='ward')\n",
    "\n",
    "# plot dendrogram\n",
    "plt.figure(figsize=(14, 8))\n",
    "dendro = dendrogram(\n",
    "    Z,\n",
    "    labels=z_scores['text'].values,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    ")\n",
    "\n",
    "# color leaves by author\n",
    "ax = plt.gca()\n",
    "author_list = z_scores['authors'].tolist()\n",
    "unique_authors = z_scores['authors'].unique()\n",
    "colors = plt.cm.tab10(range(len(unique_authors)))\n",
    "author_colors = dict(zip(unique_authors, colors))\n",
    "\n",
    "for tick_label in ax.get_xticklabels():\n",
    "    text = tick_label.get_text()\n",
    "    author = z_scores.loc[z_scores['text'] == text, 'authors'].values[0]\n",
    "    tick_label.set_color(author_colors[author])\n",
    "\n",
    "plt.title(\"Dendrogram of texts based on POS bigram z-scores (cosine distance)\")\n",
    "plt.xlabel(\"Text\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#setting a threshhold of 2 for z score, the result is the same\n",
    "\n",
    "#print(X)\n",
    "z_filtered = X.copy()\n",
    "z_filtered = z_filtered.mask(z_filtered.abs() >= 2,0)\n",
    "\n",
    "\n",
    "# Compute cosine similarity and distance\n",
    "sim_matrix = cosine_similarity(z_filtered)\n",
    "dist_matrix = 1 - sim_matrix\n",
    "\n",
    "# Ensure diagonal is exactly zero\n",
    "np.fill_diagonal(dist_matrix, 0)\n",
    "\n",
    "# Flatten for linkage\n",
    "dist_condensed = squareform(dist_matrix)\n",
    "\n",
    "\n",
    "# Hierarchical clustering\n",
    "Z = linkage(dist_condensed, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(14, 8))\n",
    "dendro = dendrogram(\n",
    "    Z,\n",
    "    labels=z_scores['text'].values,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    ")\n",
    "\n",
    "# Color leaves by author\n",
    "ax = plt.gca()\n",
    "author_list = z_scores['authors'].tolist()\n",
    "unique_authors = z_scores['authors'].unique()\n",
    "colors = plt.cm.tab10(range(len(unique_authors)))\n",
    "author_colors = dict(zip(unique_authors, colors))\n",
    "\n",
    "for tick_label in ax.get_xticklabels():\n",
    "    text = tick_label.get_text()\n",
    "    author = z_scores.loc[z_scores['text'] == text, 'authors'].values[0]\n",
    "    tick_label.set_color(author_colors[author])\n",
    "\n",
    "plt.title(\"Dendrogram of texts based on POS bigram z-scores (cosine distance)\")\n",
    "plt.xlabel(\"Text\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''\n",
    "#PCA for cos_sim\n",
    "\n",
    "# sim_matrix: cosine similarity matrix (numpy array)\n",
    "# z_scores['text'] contains text names\n",
    "# z_scores['authors'] contains authors\n",
    "\n",
    "# Convert similarity to distance for PCA: distance = 1 - similarity\n",
    "dist_matrix = 1 - sim_matrix\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(dist_matrix)\n",
    "\n",
    "# Make a DataFrame for plotting\n",
    "df_plot = pd.DataFrame({\n",
    "    'PC1': coords[:, 0],\n",
    "    'PC2': coords[:, 1],\n",
    "    'text': z_scores['text'],\n",
    "    'author': z_scores['authors']\n",
    "})\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for author in df_plot['author'].unique():\n",
    "    subset = df_plot[df_plot['author'] == author]\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=author)\n",
    "\n",
    "# Add annotations (play names) for each point\n",
    "for i, row in df_plot.iterrows():\n",
    "    plt.text(row['PC1'] + 0.02, row['PC2'] + 0.02, row['text'], fontsize=8)  \n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA of texts based on high z-score POS bigrams\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pca analysis on all grams\n",
    "\n",
    "#combine POS uni-, bi-, and trigram frequencies\n",
    "X = {}\n",
    "for name in combined_data['names']:\n",
    "    X[name] = copy.deepcopy(pos_frequencies[name])      # Start with unigrams\n",
    "    X[name].update(pos_bigrams[name])                   # Add bigrams\n",
    "    X[name].update(pos_trigrams[name])                  # Add trigrams\n",
    "\n",
    "#normalize keys: turn tuples into strings\n",
    "def string_key(k):\n",
    "    if isinstance(k, str):\n",
    "        return k\n",
    "    return \">\".join(k)  # Join POS tags with '>' to mark order\n",
    "\n",
    "X_str_keys = {}\n",
    "for name, feats in X.items():\n",
    "    X_str_keys[name] = {string_key(k): v for k, v in feats.items()}\n",
    "\n",
    "#build DataFrame\n",
    "df_features = pd.DataFrame.from_dict(X_str_keys, orient=\"index\").fillna(0)\n",
    "\n",
    "print(\"Feature matrix shape:\", df_features.shape)\n",
    "print(\"Sample columns:\", df_features.columns[:-10])\n",
    "\n",
    "#apply PCA\n",
    "pca = PCA(n_components=2) #collapse featrues into 2 dimensions for easy visualisation\n",
    "principal_components = pca.fit_transform(df_features)\n",
    "\n",
    "pca_df = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df['authors']=combined_data['authors']\n",
    "pca_df['years']=combined_data['years']\n",
    "pca_df['names']=df_features.index\n",
    "\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Scatter plot\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.scatterplot(\n",
    "    data=pca_df,\n",
    "    x=\"PC1\", y=\"PC2\",\n",
    "    hue=\"years\",                # Color points by year , shape by author\n",
    "    palette=\"viridis\",\n",
    "    style='authors',\n",
    "    \n",
    "    s=300\n",
    ")\n",
    "\n",
    "#add name labels for each point\n",
    "for i, row in pca_df.iterrows():\n",
    "    plt.text(\n",
    "        row[\"PC1\"] + 0.002,      # small horizontal offset\n",
    "        row[\"PC2\"] + 0.002,      # small vertical offset\n",
    "        row[\"names\"],            # column in pca_df with the play name\n",
    "        fontsize=10,\n",
    "        color=\"black\"\n",
    "    )\n",
    "\n",
    "\n",
    "plt.title(\"PCA of POS N-gram Frequencies\", fontsize=16)\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
    "plt.axhline(0, color=\"grey\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.axvline(0, color=\"grey\", linewidth=0.5, linestyle=\"--\")\n",
    "plt.legend(title=\"plays\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
